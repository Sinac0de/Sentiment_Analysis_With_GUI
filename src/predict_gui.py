import streamlit as st
import torch
from transformers import AutoTokenizer
from enhanced_model import EnhancedPersianSentimentModel
from enhanced_preprocessing import EnhancedPersianPreprocessor
import os
import numpy as np
import random
import pandas as pd

MODEL_PATH = 'models/enhanced_model/best_model.pth'
MODEL_NAME = 'HooshvareLab/bert-fa-base-uncased'
TEST_DATA_PATH = 'data/processed_data/test_enhanced.csv'


@st.cache_resource
def load_model_and_tokenizer():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = EnhancedPersianSentimentModel(
        MODEL_NAME, num_classes=3, dropout_rate=0.3)
    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    model.to(device)
    model.eval()
    return model, tokenizer, device


preprocessor = EnhancedPersianPreprocessor()
model, tokenizer, device = load_model_and_tokenizer()
class_names = ['Ù…Ù†ÙÛŒ', 'Ø®Ù†Ø«ÛŒ', 'Ù…Ø«Ø¨Øª']

st.set_page_config(page_title="ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª ÙØ§Ø±Ø³ÛŒ", page_icon="ğŸ’¬")
st.title("ğŸ’¬ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ Ù…Ø¯Ù„ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ")
st.write("ÛŒÚ© Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ ØªØ§ Ø§Ø­Ø³Ø§Ø³ Ø¢Ù† (Ù…Ø«Ø¨ØªØŒ Ù…Ù†ÙÛŒ ÛŒØ§ Ø®Ù†Ø«ÛŒ) Ø±Ø§ Ù…Ø¯Ù„ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù†Ø¯.")

# --- Prediction History ---
if 'history' not in st.session_state:
    st.session_state['history'] = []

user_text = st.text_area("Ù…ØªÙ† Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯:", height=120)
col1, col2 = st.columns([1, 1])
predict_btn = col1.button("Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ø­Ø³Ø§Ø³")
clear_btn = col2.button("Ù¾Ø§Ú©â€ŒÚ©Ø±Ø¯Ù† ØªØ§Ø±ÛŒØ®Ú†Ù‡")

if clear_btn:
    st.session_state['history'] = []
    st.info("ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù¾Ø§Ú© Ø´Ø¯.")

if predict_btn:
    if not user_text.strip():
        st.warning("Ù„Ø·ÙØ§Ù‹ ÛŒÚ© Ù…ØªÙ† ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.")
    else:
        try:
            with st.spinner("Ø¯Ø± Ø­Ø§Ù„ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ..."):
                processed = preprocessor.preprocess_text(user_text)
                encoding = tokenizer(
                    processed,
                    truncation=True,
                    padding='max_length',
                    max_length=128,
                    return_tensors='pt'
                )
                input_ids = encoding['input_ids'].to(device)
                attention_mask = encoding['attention_mask'].to(device)
                with torch.no_grad():
                    logits = model(input_ids=input_ids,
                                   attention_mask=attention_mask)
                    probs = torch.softmax(logits, dim=1).cpu().numpy()[0]
                    pred = int(np.argmax(probs))
                st.success(f"Ù†ØªÛŒØ¬Ù‡: **{class_names[pred]}**")
                st.info(f"Ù…ØªÙ† Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡: {processed}")
                st.subheader("Ø§Ø­ØªÙ…Ø§Ù„ Ù‡Ø± Ú©Ù„Ø§Ø³:")
                st.bar_chart({"Ø§Ø­ØªÙ…Ø§Ù„": probs}, use_container_width=True)
                # Add to history
                st.session_state['history'].append({
                    'Ù…ØªÙ†': user_text,
                    'Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ': class_names[pred],
                    'Ø§Ø­ØªÙ…Ø§Ù„ Ù…Ù†ÙÛŒ': f"{probs[0]*100:.1f}%",
                    'Ø§Ø­ØªÙ…Ø§Ù„ Ø®Ù†Ø«ÛŒ': f"{probs[1]*100:.1f}%",
                    'Ø§Ø­ØªÙ…Ø§Ù„ Ù…Ø«Ø¨Øª': f"{probs[2]*100:.1f}%"
                })
        except Exception as e:
            st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ: {e}")

# --- Prediction History Table ---
if st.session_state['history']:
    st.markdown("---")
    st.subheader("ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ (Ø§ÛŒÙ† Ø¬Ù„Ø³Ù‡):")
    st.dataframe(pd.DataFrame(st.session_state['history']))

st.markdown("---")
st.caption("Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ Ø¨Ø§ â¤ï¸ ØªÙˆØ³Ø· Ù…Ø¯Ù„ ParsBERT Ùˆ ØªÛŒÙ… Ø´Ù…Ø§ | Ù†Ø³Ø®Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡ GUI")
